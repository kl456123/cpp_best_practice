syntax="proto3";
import "step_stats.proto";
import "cost_graph.proto";
import "graph.proto";



message ConfigProto{
    map<string, int32> device_count=1;
    // Whether device placements should be logged.
    bool log_device_placement = 8;
    // Options that apply to all graphs.
    GraphOptions graph_options = 10;

    repeated ThreadPoolOptionProto session_inter_op_thread_pool = 12;

    // The execution of an individual op (for some op types) can be
    // parallelized on a pool of intra_op_parallelism_threads.
    // 0 means the system picks an appropriate number.
    //
    // If you create an ordinary session, e.g., from Python or C++,
    // then there is exactly one intra op thread pool per process.
    // The first session created determines the number of threads in this pool.
    // All subsequent sessions reuse/share this one global pool.
    //
    // There are notable exceptions to the default behavior describe above:
    // 1. There is an environment variable  for overriding this thread pool,
    //    named TF_OVERRIDE_GLOBAL_THREADPOOL.
    // 2. When connecting to a server, such as a remote `tf.train.Server`
    //    instance, then this option will be ignored altogether.
    int32 intra_op_parallelism_threads = 2;
    // Nodes that perform blocking operations are enqueued on a pool of
    // inter_op_parallelism_threads available in each process.
    //
    // 0 means the system picks an appropriate number.
    // Negative means all operations are performed in caller's thread.
    //
    // Note that the first Session created in the process sets the
    // number of threads for all future sessions unless use_per_session_threads is
    // true or session_inter_op_thread_pool is configured.
    int32 inter_op_parallelism_threads = 5;
}

message RunMetadata{
    StepStats step_stats=1;
    CostGraphDef cost_graph =2;

    repeated GraphDef partition_graphs = 3;
}
message SessionMetadata{
    string name=1;
    int64 version=2;
}

message RunOptions{
    enum TraceLevel{
        NO_TRACE=0;
        SOFTWARE_TRACE=1;
        HARDWARE_TRACE=2;
        FULL_TRACE=3;
    }
    TraceLevel trace_level=1;

    int32 inter_op_thread_pool=3;
    bool output_partition_graphs =5;
}


message ThreadPoolOptionProto{
    int32 num_threads=1;
    string global_name =2;
}

message CallableOptions{
    repeated string feed=1;
    repeated string fetch=2;
    repeated string target=3;
    RunOptions run_options=4;
    // The Tensor objects fed in the callable and fetched from the callable
    // are expected to be backed by host (CPU) memory by default.
    //
    // The options below allow changing that - feeding tensors backed by
    // device memory, or returning tensors that are backed by device memory.
    //
    // The maps below map the name of a feed/fetch tensor (which appears in
    // 'feed' or 'fetch' fields above), to the fully qualified name of the device
    // owning the memory backing the contents of the tensor.
    //
    // For example, creating a callable with the following options:
    //
    // CallableOptions {
    //   feed: "a:0"
    //   feed: "b:0"
    //
    //   fetch: "x:0"
    //   fetch: "y:0"
    //
    //   feed_devices: {
    //     "a:0": "/job:localhost/replica:0/task:0/device:GPU:0"
    //   }
    //
    //   fetch_devices: {
    //     "y:0": "/job:localhost/replica:0/task:0/device:GPU:0"
    //  }
    // }
    //
    // means that the Callable expects:
    // - The first argument ("a:0") is a Tensor backed by GPU memory.
    // - The second argument ("b:0") is a Tensor backed by host memory.
    // and of its return values:
    // - The first output ("x:0") will be backed by host memory.
    // - The second output ("y:0") will be backed by GPU memory.
    //
    // FEEDS:
    // It is the responsibility of the caller to ensure that the memory of the fed
    // tensors will be correctly initialized and synchronized before it is
    // accessed by operations executed during the call to Session::RunCallable().
    //
    // This is typically ensured by using the TensorFlow memory allocators
    // (Device::GetAllocator()) to create the Tensor to be fed.
    //
    // Alternatively, for CUDA-enabled GPU devices, this typically means that the
    // operation that produced the contents of the tensor has completed, i.e., the
    // CUDA stream has been synchronized (e.g., via cuCtxSynchronize() or
    // cuStreamSynchronize()).
    map<string, string> feed_devices = 6;
    map<string, string> fetch_devices = 7;

    // By default, RunCallable() will synchronize the GPU stream before returning
    // fetched tensors on a GPU device, to ensure that the values in those tensors
    // have been produced. This simplifies interacting with the tensors, but
    // potentially incurs a performance hit.
    //
    // If this options is set to true, the caller is responsible for ensuring
    // that the values in the fetched tensors have been produced before they are
    // used. The caller can do this by invoking `Device::Sync()` on the underlying
    // device(s), or by feeding the tensors back to the same Session using
    // `feed_devices` with the same corresponding device name.
    bool fetch_skip_sync = 8;
}

message GraphOptions {
    // Removed, use optimizer_options below.
    reserved "skip_common_subexpression_elimination";
    reserved 1;

    // If true, use control flow to schedule the activation of Recv nodes.
    // (Currently ignored.)
    bool enable_recv_scheduling = 2;

    // Options controlling how graph is optimized.
    OptimizerOptions optimizer_options = 3;

    // The number of steps to run before returning a cost model detailing
    // the memory usage and performance of each node of the graph. 0 means
    // no cost model.
    int64 build_cost_model = 4;

    // The number of steps to skip before collecting statistics for the
    // cost model.
    int64 build_cost_model_after = 9;

    // Annotate each Node with Op output shape data, to the extent it can
    // be statically inferred.
    bool infer_shapes = 5;

    // Only place the subgraphs that are run, rather than the entire graph.
    //
    // This is useful for interactive graph building, where one might
    // produce graphs that cannot be placed during the debugging
    // process.  In particular, it allows the client to continue work in
    // a session after adding a node to a graph whose placement
    // constraints are unsatisfiable.
    bool place_pruned_graph = 6;

    // If true, transfer float values between processes as bfloat16.
    bool enable_bfloat16_sendrecv = 7;

    // If > 0, record a timeline every this many steps.
    // EXPERIMENTAL: This currently has no effect in MasterSession.
    int32 timeline_step = 8;

    // Options that control the type and amount of graph rewriting.
    // Not currently configurable via the public Python API (i.e. there is no API
    // stability guarantee if you import RewriterConfig explicitly).
    RewriterConfig rewrite_options = 10;
}
